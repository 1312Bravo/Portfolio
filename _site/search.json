[
  {
    "objectID": "projects/training_log.html",
    "href": "projects/training_log.html",
    "title": "Training Log",
    "section": "",
    "text": "I see everything in life through the prism of mathematics or statistics. It’s no different with sports, especially running (or cycling). This blog will be a place where I combine my passion for endurance sports with professional background in data science. Just as I enjoy solving equations or structuring data models at work, I enjoy applying that same curiosity to my own training. Training logs, race preparation, and performance modeling. All of these can be enriched by numbers, models, and careful analysis.\nAnother motivation behind starting this blog is that I recently began working with a coach. That gave me an extra push to look deeper into how we are structuring the training, how I’m progressing, and to better understand the “why” behind what we’re doing. It’s not only about following workouts but also about making sense of the bigger picture, seeing how the plan unfolds, and how the numbers reflect adaptation and improvement.\nNumbers, when put into context, tell a story: whether I’m adapting, improving, or stepping into risky territory. That’s why each blog post will focus on a specific question and explore how mathematical thinking can bring clarity, whether it’s about training load, heart rate, distance, or other aspects of performance. This blog is my way of sharing that journey, where mountains, trails, and bikes meet mathematics, analytics, and a touch of artistry.\n\n    \n     Check out the full \n     GitHub repository \n    for this project\n\n\nTrack Your Training Smarter - Data\nKeep all your training and daily metrics in one place without lifting a finger. With a Python pipeline connecting Garmin Connect to Google Sheets, your workouts and daily metrics update automatically every day. The result: a complete, always-current record that makes analysis, trend spotting, and make data-driven decisions precise. 2025-08-20\n\n\nIncrease your chances of staying away from running injuries - HASR-TL\nPut recent training into context by comparing it with your history. Give yourself a clearer picture of whether you’re pushing into new territory or staying within safe limits. The goal is to help you understand your training load better, catch signs of overtraining early, reduce the risk of injuries or just define where in the training cycle you are. 2025-09-15"
  },
  {
    "objectID": "projects/training_log/data.html",
    "href": "projects/training_log/data.html",
    "title": "Track Your Training Smarter - Data",
    "section": "",
    "text": "Before diving into analysis, it’s worth showing where the numbers come from. I wanted a consistent and automated way to capture my training.\nTwo main datasets are daily updated and live in Google Sheets. Having the data in this format makes it easy to explore manually, but also gives me a flexible base for deeper analysis. I provide only a static sample of data logs.\n\nActivity Log – training activities with different metrics, including distance, duration, elevation, heart rate.\n\n\n \n\n\nDaily Log – day-level statistics, such as steps, sleep, resting heart rate, recovery time, and other daily metrics.\n\n\n \n\nThe statistics come from the Garmin Connect API. A Python script fetches both activity and daily statistics, and then pushes them into the Google Sheets. Some metrics are raw Garmin values, while others are adjusted or calculated. Statistics that are captured in sheets can also evolve over time as we discover new metrics or further analyze training. So, some of the statistics in the sheet can also be presented in details in other blog posts.\nThe script is run each morning and spreadsheets expand by one more day of history (yesterday’s data), keeping the logs synchronized without manual work. The script can also loop through multiple accounts if provided.\nThe script relies two key integrations:\n\nGarmin Connect API – fetch activities and health metrics.\n\nfrom garminconnect import Garmin  \n\ngarminClient = Garmin(garmin_email, garmin_password)  \ngarminClient.login()\ngarminClient.get_activities_by_date(date_from, date_to)\n\nGoogle Sheets API – store and update logs.\n\nimport gspread\nfrom google.oauth2.service_account import Credentials  \n\ndrive_credentials = Credentials.from_service_account_file(\n    \"googleDrive_secrets.json\",  \n    scopes=[\"https://www.googleapis.com/auth/spreadsheets\",  \n            \"https://www.googleapis.com/auth/drive\"]  \n)\n\ngoogleDrive_client = gspread.authorize(drive_credentials)  \ngoogleDrive_client.open(user_dailyLogFilename)\nOf course, logging in requires credentials (emails, passwords, API keys). They are not hardcoded, but live in a .env file locally and as GitHub Secrets in the cloud, keeping sensitive content secure while still accessible for automation.\n\n    \n     You can find the full code for this section, which is then easily integrated into the src/main.py script, on my Github repository under the\n     basic_daily_activity_statistics\n     folder."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Urh Peček",
    "section": "",
    "text": "Hi, I’m Urh Peček! A data scientist and lifelong learner, fascinated by how numbers, models, and analytics help us make sense of the world. On this site, I share projects, experiments, and stories that spark my curiosity.\nThese posts come from my own independent interests and questions I want to explore, datasets I find intriguing, or challenges that push me to think differently. Each project may be explored through multiple blog-style posts, each focusing on a different aspect, insight, or experiment. This allows me to break complex topics into digestible pieces while following a clear narrative. Each one is an opportunity to apply mathematics, statistics, and analytical thinking in a way that’s both rigorous and creative.\nI aim to make insights clear, actionable, and occasionally even artistic. I share not just results, but the thought process, experiments, and learning along the way. My hope is that every post inspires you to look at problems differently, question assumptions, and maybe spark your own experiments."
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Urh Peček",
    "section": "Featured Projects",
    "text": "Featured Projects\n\nTraining Log\nAnalysing training data with Python and Garmin data."
  },
  {
    "objectID": "projects/training_log/hasr_tl.html",
    "href": "projects/training_log/hasr_tl.html",
    "title": "Increase your chances of staying away from running injuries - HASR-TL",
    "section": "",
    "text": "Idea\nWhen training for endurance sports, our bodies adapt over time, and whether a workout is “hard” or “easy” on the body, depends on what we’ve been doing recently and historically. The goal is to define a simple metric that shows where we are with our current training compared to what we’ve been doing in the recent past. The purpose is to:\n\nsee if we should reduce our training load so that we don’t overreach or risk injury,\noncrease it to match what our body has adapted to in the recent past, or\nsimply define where we are in the training cycle when taking the bigger picture into account.\n\nThe idea of comparing current workload to past workload isn’t new. Sports science has long used the Acute:Chronic Workload Ratio (ACWR) to capture how today’s training compares to longer-term training history. But ACWR has well-known limitations: it usually relies on a single rolling average, and it can miss important context about peak efforts or variation in load. That’s where our approach comes in. We’ll further extend it into a more flexible and informative framework.\n\n\nSelecting the metric:\nTo measure effort, we focus on Training Load, a metric that combines all other measures (options: distance, time, heart rate, pace or something else) into a single number. This also strongly coincides with the sport we are analyzing - trail running, where it is hard to judge intensity from speed or distance alone. We might talk more about Training Load in further blogs.\nTraining Load is provided from all of ours smart wearables and a practical way for making comparisons across very different sessions (and sports). We will use Garmin’s Training Load values provided in our training log.\nThe same approach can be applied to any other training metric. And by calculating values for each, we can see the story from multiple angles.\n\n\nDeep dive\nLet \\(TL_i\\) be the training load of the day \\(i\\). Our goal is to define a metric that normalizes recent TL against historical. We can think of this in two complementary components:\n\nBaseline (long term) adaptation - Load the body has been succesfully adapted to over a longer period, assuming this distribution is “safe”, disregarding recent acute load.\n\nDenoted as \\(TL_{\\text{baseline},t}\\).\nComputed over a long baseline window of \\(N\\) days, excluding last \\(n\\) days: \\[\\mathcal{L}_t=\\{TL_{t-(n+j)}\\mid j=1,\\dots, N\\}\\]\nTo give more importance to recent training within the baseline period, we assign weights decreasing with days: \\[w_j = \\lambda^{j-1}, \\quad 0 &lt; \\lambda &lt;= 1, \\quad j = 1, ..., N,\\] where more recent baseline days contribute more to defining the baseline load.\n\nRecent training pattern - Load the body is currently being exposed to, capturing acute training load.\n\nDenoted as \\(TL_{\\text{recent},t}\\).\nComputed over a recent window of \\(n\\) days: \\[\\mathcal{R}_t=\\{TL_{t-j}\\mid j=1,\\dots,n\\}\\].\nThis can also be weighted to emphasize the most recent sessions: \\[v_j = \\lambda^{j-1}, \\quad 0 &lt; \\lambda &lt;= 1, \\quad j = 1, ..., n\\]\n\n\nWe define the parameters as:\n\nBaseline window: \\(N = 90\\) days\nRecent window: \\(n = 14\\) days\nWeight base \\(\\lambda\\) = \\((0.5)^\\frac{1}{31} = 0.978\\), so weight halves approximately every 31 days.\n\nAdditionaly: Because we aim to capture relative load patterns rather than total accumulated load, we normalize the weights so that they sum to 1. This ensures that the weighted, for example averages, for baseline and recent windows are directly comparable:\n\n\\(\\overline{w}_j = \\frac{w_j}{\\sum_{k=1}^{N}w_k} \\quad j = 1, ..., N\\)\n\\(\\overline{v}_j = \\frac{v_j}{\\sum_{k=1}^{v}w_k} \\quad j = 1, ..., v\\)\n\n\nNotes: By keeping the baseline and recent windows non-overlapping, we ensure that the baseline reflects only training the body has already adapted to, without being influenced by recent sessions that the body has not yet adjusted to. This allows us to identify increases in stress in the recent window that may pose a risk.\nDataset: We take into account all measured activities, regardless if it was real training or not (including hiking, swimming, easy cycling etc.) and treat total daily training load as daily sample.\n\n\nPercentile stratified metric\nIn endurance sports, training days can be grouped into a few main types:\n\nEasy sessions & Rest days - used for recovery, aerobic base, and technique work. These make up roughly 55% of all sessions.\nHard sessions - tempo, threshold, VO2max, or interval workouts. Typically around 30% of sessions.\nLong days - the occasional very long run, bike ride, or race that forms the extreme right tail of the distribution. About 15% of sessions.\n\nBy tracking these session types separately we can see if:\n\neasy sessins are getting too long, too intense, or too rare, ensuring that we can perform well in harder workouts,\nor recent hard sessions make up a reasonable portion of total sessions to allow sufficient recovery,\nor recent long days are not too frequent or extreme.\n\nWe will divide our days into these three categories based on Training Load values. Baseline window training load percentiles to define thresholds as this approach is individualized, automatic, and reproducible. These thresholds are then applied to both baseline and recent data.\n\nThis ensures the baseline distribution reflects the training the body has already adapted to, providing a safe reference.\nRecent sessions are evaluated relative to this safe baseline, so any increase in intensity, frequency, or duration signals higher acute load or potential risk.\n\nFormally, let \\(\\mathcal{Q}^w_p(\\mathcal{L}_t)\\) be weighted \\(p\\)-th quantile of the baseline window \\(\\mathcal{L}_t\\) using weights \\(w_j\\) and define the baseline and recent buckets using baseline percentiles \\(q^w_{70,t}\\) and \\(q^w_{90,t}\\) as follows:\n\nEasy sessions & Rest days: Training loads falling below the weighted \\(q_{low}\\) percentile of the window: \\[TL \\leq q^w_{low,t}=Q^w_{low}(\\mathcal{L}_t)\\]\nHard sessions: Training loads between the weighted \\(q_{low}\\) and \\(q_{high}\\) percentiles: \\[q^w_{70,t} &lt; TL \\leq q^w_{high,t}=Q^w_{high}(\\mathcal{L}_t)\\]\nLong days: Training loads above the weighted 90th percentile: \\[TL &gt; q^w_{high,t}=Q^w_{high}(\\mathcal{L}_t)\\]\n\nClarifying notes:\n\nIn order to perform any structured analysis, each session must be assigned to a bucket based on the percentile thresholds. This means that some sessions may fall into different buckets due to small differences in Training Load, even if their physiological impact is similar. So these categories should be interpreted with caution. Categories are a practical tool for analysis and a guide, rather than absolute labels. When interpreting them, consider the broader context.\nThe distinction created by quantiles: that hard sessions have lower training load than long sessions, is based on observed patterns in my training. In particular, for ultra and sub-ultra endurance activities such as trail running, long sessions are disproportionately long and thus accumulate higher training loads compared to interval-based hard sessions. If different training pattern would be observed, the classification scheme could be adjusted accordingly. Also, in some cases of extremely hard sessions, the accumulated training load may be high enough that they fall into the “long” classification bucket, even though their nature differs from typical long endurance efforts.\n\nNote on weighted percentiles: The weighted percentile represents the TL at which the cumulative sum of weights reaches the desired fraction of total weight. This accounts for the fact that more recent baseline sessions contribute more to the threshold.\nHaving defined these buckets, we summarize each bucket by the weighted average training load within, where we allow more recent training days to contribute more to the bucket averages. With this, we make the metric sensitive to shifts in the typical intensity of each type of session within the bucket.\nBased on my training data (with a hardcoded mapping into the categories Rest, Easy, Hard, Long, and Other), the distribution of training loads and their descriptive values are as follows:\n\nRest - 14% of days, TL = avg 0, SD 0\nEasy - 47% of days, TL = avg 96, SD 43\nHard - 14% of days, TL = avg 190, SD 56\nLong - 18% of days, TL = avg 218, SD 96\nOther - 7% of days, TL = avg 130, SD 66\n\n\nLet \\(w_j\\) be the weight of day \\(j\\) in the baseline window \\(\\mathcal{L}_t\\) and \\(v_j\\) be the weight of day \\(j\\) in the recent window \\(\\mathcal{R}_t\\). We then define the weighted averages within each bucket as:\nBaseline bucket weighted averages:\n\\[\\text{Easy}: \\mu^w_{1,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{L}_t,\\; TL \\le q^w_{low,t}\\,]\\] \\[\\text{Hard}: \\mu^w_{2,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{L}_t,\\; q^w_{low,t} &lt; TL \\le q^w_{high,t}\\,]\\] \\[\\text{Long}: \\mu^w_{3,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{L}_t,\\; TL &gt; q^w_{high,t}\\,]\\]\nRecent bucket weighted averages:\n\\[\\text{Easy}: \\nu^w_{1,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{R}_t,\\; TL \\le q^w_{low,t}\\,]\\] \\[\\text{Hard}: \\nu^w_{2,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{R}_t,\\; q^w_{low,t} &lt; TL \\le q^w_{high,t}\\,]\\] \\[\\text{Long}: \\nu^w_{3,t} = \\mathbb{E}_w[\\,TL \\mid TL \\in \\mathcal{R}_t,\\; TL &gt; q^w_{high,t}\\,]\\]\nWhere \\(\\mathbb{E}[\\cdot]\\) denotes the empirical weighted mean \\(\\mathbb{E}_w[v] = \\frac{\\sum_{i=1}^nw_iv_i}{\\sum_{i=1}^nw_i}\\) over the subset of training load values falling into the specified bucket.\nIn addition to the weighted averages, we can also track the proportion of sessions falling into each bucket in both the baseline and the recent window. Wheres the proportion of sessions falling into each bucket in Baseline window as pre-determined with quantiles, the number of easy, hard, and long sessions is not fixed once baseline thresholds are applied to recent training. For example, if the proportion of easy days decreases in recent window may signal insufficient recovery.\nFormally, let: \\[\\pi_{k,t}^{(b)} \\quad \\text{and} \\quad \\pi_{k,t}^{(r)}\\] denote proportions of sessions in bucket \\(k = 1,2,3\\) in baseline and recent window respectively.\nMost recent Session Classification\nIn addition to aggregated metrics, we can also assess each most recent individual session in the context of the baseline distribution. Specifically, for the most recent training session (e.g., the date of analysis), we can determine where its Training Load falls within the weighted baseline distribution.\nThis can be expressed as the weighted percentile (or baseline-relative quantile rank) of the session:\n\\[\\phi_t = \\frac{\\sum_{j \\in \\mathcal{L}_t} w_j \\,\\mathbf{1}\\{\\,TL_j \\leq TL_t^{(\\text{recent})}\\}}{\\sum_{j \\in \\mathcal{L}_t} w_j}\\]\nThis allows us to assign the session to one of the predefined buckets: Easy, Hard, or Long - using the same percentile thresholds derived from the baseline window:\n\\[\n\\text{Session classification} =\n\\begin{cases}\n\\text{Easy} & \\text{if } TL \\le q^w_{\\text{low},t} \\\\[2mm]\n\\text{Hard} & \\text{if } q^w_{\\text{low},t} &lt; TL \\le q^w_{\\text{high},t} \\\\[1mm]\n\\text{Long} & \\text{if } TL &gt; q^w_{\\text{high},t}\n\\end{cases}\n\\]\nAlso, as mentioned above, these classifications are meant as guidance; a more complete understanding comes from considering the weighted percentile values, which provide richer context on the session’s relative intensity.\nBy doing this, we can assess the acute characteristics of the latest session relative to what the athlete has already adapted to and estimate how individual session contributes to overall training stress. This information can then guide adjustments for upcoming workouts to ensure we stay on track with the training plan.\nApplied recent session classification analysis\nBased on the applied most recent session classification, we evaluated the alignment between the model’s classifications and the training type categories that were assigned “by hand”, presented above.\nIt is important to emphasize that a mismatch between the two does not necessarily mean the “model” was wrong. Instead, it may indicate that the most recent session was actually harder or easier than we would perceive it. Another possibility is that the sessions in the baseline window were skewed toward the easier or harder side, which would shift the distribution and make the thresholds less “evenly” balanced across truly easy, hard, and long days.\n\nEasy - proportion of sessions classifed as “by hand” = 90%, average baseline-relative quantile rank = 0.32\nHard - proportion of sessions classifed as “by hand” = 31%, average baseline-relative quantile rank = 0.72\nLong = proportion of sessions classifed as “by hand” = 53%, average baseline-relative quantile rank = 0.93\n\nMost of the hard classified sessions that were not classifed the same as “by hand” were some “easy” days that had some fast strides in and some medium-long sessions, that we classifed as long by hand. For the long sessions, most of the wrongly classifed were “by hand” perceived hard, but with training load closer to long runs.\nMost of the sessions classified as Hard that did not match the hand-labeled categories were actually “easier” days that included some fast strides or longer high paced segments. Other were some medium-long sessions which were manually classified as Long. For the Long sessions, the majority of misclassifications involved sessions that were perceived as Hard by hand, but whose overall Training Load was closer to that of typical Long sessions, so really hard, fast sessions.\n\n\nHistory-Aware Statified Relative - Training Load (HASR-TL)\nOnce training sessions are divided into the three percentile-based buckets, defined from the baseline window, we can combine them into a single, interpretable metric that reflects how recent training compares to the athlete’s long-term adaptation. The central idea is weighted aggregation, where each bucket contributes differently to the overall metric. By aggregating the per-bucket loads with appropriate weights, HASRTL produces a single number that reflects how the overall recent training load compares to the baseline adaptation.\nWeighted aggregation\nLet \\(w_1, w_2, w_3\\) denote weights for easy, hard, and long sessions, respectively, with \\[w_1+w_2+w_3=1,\\quad w_k\\ge 0\\]. The baseline aggregate load is then: \\[TL_{\\text{baseline},t} = w_1 \\mu_{1,t} + w_2 \\mu_{2,t} + w_3 \\mu_{3,t}.\\] Similarly, the recent aggregate load is: \\[TL_{\\text{recent},t} = w_1 \\nu_{1,t} + w_2 \\nu_{2,t} + w_3 \\nu_{3,t}.\\]\nFinally, the History-Aware Stratified Relative Training Load (HASRTL) is defined as: \\[\\Delta_t = \\frac{TL_{\\text{recent}}}{TL_{\\text{baseline}}}.\\]\nHere, \\(\\Delta_t\\) expresses how the current (recent) training load compares to the long-term baseline. It can be interpreted as a percentage increase or decrease of recent load relative to baseline adaptation.\nSince recent sessions are classified using baseline-defined buckets, \\(\\Delta_t\\) represents the relative training load compared to what the athlete has already adapted to, making it a robust measure of both increased stress and potential risk.\nSelection of weights \\(w_1, w_2, w_3\\)\nWhen defining the buckets weights, for our purpose, we have to consider the bucket’s contribution to adaptation and also their potential impact on injury risk. The weights should reflect the relative importance of bucket’s session type in promoting positive training effects while also accounting for their role in acute stress spikes that may increase the probability of injury.\n\nEasy sessions: As we defined them, they are frequent and involve low-intensity activities. They have low impact on both the adaptation and injury risk. Weight reflects their impact on training adaptation and injury risk.\n\n\\(w_1 = 0.15\\)\n\nHard sessison: Less frequent but high-intensity activities that drive the adaptation and also impose higher acute loads that increase injury risk. Weight reflects their contribution to acute stress.\n\n\\(w2 = 0.40\\)\n\nLong sessison: They are rare but lead to high cumulative stress. They significantly contribute to adaptation and also pose a highest risk of injury. Weight reflects their peak stress contribution.\n\n\\(w3 = 0.45\\)\n\n\nThese weights are not directly measurable, they are largely subjective, based on expert knowledge, experience, and general principles of training load management rather than precise data.\n\n\nInterpretation & Bucket-level diagnostics\nAlthough \\(\\Delta_t\\) provides a clear mathematical ratio of recent to baseline training load, its practical interpretation is less straightforward - it is not always obvious when an increase or decrease becomes significant, risky, or indicative of detraining.\n\nThe idea behind the HASR-TL metric is to summarize this relationship in a single number, but a deeper understanding comes from also examining the individual training buckets (e.g., easy, hard, long). Comparing these buckets within the baseline and recent windows, and between them, helps reveal where changes originate and adds richer context than the ratio alone.\nBy comparing bucket values within the same window, we gain insight to how well the training program is balanced across session types. This helps us check whether easy, hard, and long sessions are clearly distinct, or whether the loads are starting to blur together, which could reduce the effectiveness of the training. In practice, we calculate pairwise ratios of bucket weighted averages:\n\\[\\zeta_{\\mu_i, \\mu_j} = \\frac{\\mu_{i,t}}{\\mu_{j,t}} \\quad \\text{and} \\quad \\zeta_{\\nu_i, \\nu_j} = \\frac{\\nu_{i,t}}{\\nu_{j,t}}, \\quad i,j = 1,2,3, i \\neq j\\]\nBy monitoring these values, we can identify trends such as if easy vs. hard sessions are becoming too similar, which might indicate reduced stimulus diversity.\n\nTo understand which types of sessions are driving changes in overall load and what type of training have we been emphasizing lately, we compare values each bucket between recent and baseline windows.\n\\[\\delta_{k,t} = \\frac{\\nu_{k,t}}{\\mu_{k,t}}, \\quad k = 1,2,3\\]\nThis ratio focuses on the intensity or load of sessions of a given type, independent of how often these sessions occur.\n\nFor example, a rise in \\(\\delta_{1,t}\\) might indicate that easy sessions are becoming more demanding relative to the baseline. Whereas a decrease in \\(\\delta_{3,t}\\) suggests that long sessions are less intense, long possibly absent.\nIn contrast, to see how the distribution of session types is shifting, we compare the proportion of each session type in the recent window relative to the baseline:\n\\[\\rho_{k,t} = \\frac{\\pi_{k,t}^{(r)}}{\\pi_{k,t}^{(b)}}, \\quad k=1,2,3\\]\nThis ratio captures frequency changes, not intensity.\n\nFor example, \\(\\rho_{1,t} &lt; 1\\) may indicate that recovery-oriented easy days are becoming too rare, while \\(\\rho_{3,t} &gt; 1\\) suggests long sessions are occurring more frequently, than the body is adapted to.\nTogether, these diagnostics offer a comprehensive view of training composition and its evolution and form the foundation for interpreting the aggregated HASRTL metric. However, the key question remains: how large a change is meaningful, risky, or indicative of under or over-training? These interpretive challenges will be explored in the next chapter.\n\n\nImplementation\nThese statistics are similar to activity statistics presented in the Data blog, daily calculated and available for review and possible analysis in Google sheets. They are under Activity Log file - HASR-TL sheet.\n\n \n\n\n    \n     You can find the full code for this section, along with the analyses, which is then easily integrated into the src/main.py script, on my Github repository under the\n     history_aware_relative_stratified_training_load\n     folder."
  }
]